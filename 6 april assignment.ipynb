{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08565e15-2258-4126-8294-28e5c2dd1d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "A linear SVM (Support Vector Machine) is a type of SVM that uses a linear function to separate the classes. The mathematical formula for a linear SVM is:\n",
    "\n",
    "\n",
    "f(x) = sign(w^T x + b)\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "x is the input vector\n",
    "w is the weight vector\n",
    "b is the bias term\n",
    "sign() is the sign function that returns -1 if the argument is negative, 0 if the argument is zero, and 1 if the argument is positive.\n",
    "\n",
    "The goal of the linear SVM is to find the optimal values of w and b that maximize the margin between the two classes while minimizing the classification error. This can be formulated as an optimization problem that involves minimizing ||w|| subject to y_i(w^T x_i + b) >= 1 for all training examples (x_i, y_i), where y_i is the class label of example i (either -1 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ff437-5d7c-4a10-9cc5-56bc17cd44bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "The objective function of a linear SVM is to find the optimal hyperplane that maximizes the margin between the two classes while minimizing the classification error. The objective function of a linear SVM is formulated as an optimization problem that involves minimizing ||w|| subject to y_i(w^T x_i + b) >= 1 for all training examples (x_i, y_i), where y_i is the class label of example i (either -1 or 1).\n",
    "\n",
    "\n",
    "The objective function can be written as:\n",
    "\n",
    "\n",
    "minimize: 1/2 ||w||^2\n",
    "\n",
    "\n",
    "subject to: y_i(w^T x_i + b) >= 1 for all i\n",
    "\n",
    "\n",
    "where ||w|| is the Euclidean norm of the weight vector w, and the term 1/2 ||w||^2 is used to simplify the optimization problem. The constraint y_i(w^T x_i + b) >= 1 ensures that all training examples are correctly classified with a margin of at least 1. The margin is defined as the distance between the hyperplane and the closest data point from either class. The optimal hyperplane is found by solving this optimization problem using techniques such as quadratic programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d702e4f-17b6-4c62-8dc0-d67632e4b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "In SVM (Support Vector Machine), support vectors are the data points that lie closest to the decision boundary or hyperplane, and they play a crucial role in defining the decision boundary. These are the data points that have the largest margin or distance from the hyperplane, and they are the ones that determine the position and orientation of the hyperplane.\n",
    "\n",
    "\n",
    "The role of support vectors in SVM can be explained with an example. Consider a binary classification problem where we have two classes of data points, red and blue, as shown in the figure below.\n",
    "\n",
    "\n",
    "SVM Example\n",
    "\n",
    "\n",
    "In this example, we want to find a decision boundary or hyperplane that separates the two classes. The optimal hyperplane is the one that maximizes the margin between the two classes while minimizing the classification error. The margin is defined as the distance between the hyperplane and the closest data point from either class.\n",
    "\n",
    "\n",
    "In SVM, only a subset of training data points, called support vectors, are used to define the hyperplane. In this example, there are three support vectors, one blue and two red, which lie closest to the decision boundary. These support vectors determine the position and orientation of the hyperplane.\n",
    "\n",
    "\n",
    "SVM Example with Support Vectors\n",
    "\n",
    "\n",
    "The other data points that are not support vectors do not affect the position or orientation of the hyperplane. This is because they do not contribute to the margin or distance between the hyperplane and the closest data point from either class.\n",
    "\n",
    "\n",
    "Therefore, in SVM, only a small subset of training data points (support vectors) are used to define the decision boundary, which makes the algorithm computationally efficient and effective in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cd6611-5ca8-4230-827c-351c33b8d285",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sure, I can illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in SVM with examples and graphs.\n",
    "\n",
    "\n",
    "Hyperplane: In SVM, a hyperplane is a decision boundary that separates the data points into different classes. The hyperplane can be linear or nonlinear, depending on the kernel used. For example, consider the following dataset with two classes:\n",
    "\n",
    "Hyperplane Example\n",
    "\n",
    "\n",
    "In this example, a linear hyperplane can be used to separate the two classes. The hyperplane is shown as a black line in the graph below:\n",
    "\n",
    "\n",
    "Hyperplane Graph\n",
    "\n",
    "\n",
    "Marginal plane: The marginal plane is the plane that is equidistant from the support vectors on either side of the hyperplane. It is also known as the optimal separating plane. For example, consider the following dataset with two classes:\n",
    "\n",
    "Marginal Plane Example\n",
    "\n",
    "\n",
    "In this example, the marginal plane is shown as two dotted lines that are equidistant from the support vectors on either side of the hyperplane:\n",
    "\n",
    "\n",
    "Marginal Plane Graph\n",
    "\n",
    "\n",
    "Soft margin: In SVM, a soft margin allows for some misclassification of data points in order to find a better decision boundary. Soft margin SVMs are used when the data is not linearly separable. For example, consider the following dataset with two classes:\n",
    "\n",
    "Soft Margin Example\n",
    "\n",
    "\n",
    "In this example, a linear hyperplane cannot be used to separate the two classes. A soft margin SVM can be used to allow for some misclassification of data points. The decision boundary is shown as a black line with some data points misclassified:\n",
    "\n",
    "\n",
    "Soft Margin Graph\n",
    "\n",
    "\n",
    "Hard margin: In SVM, a hard margin does not allow for any misclassification of data points. Hard margin SVMs are used when the data is linearly separable. For example, consider the following dataset with two classes:\n",
    "\n",
    "![Hard Margin Example](https://miro.medium.com/max/1200/1*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce4b40c-0b88-4e27-8632-a42e53f0b609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
